{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical Vacuum Gauge Classifier\n",
    "## Given a pressure reading input returns whether it is experiencing coupling\n",
    "Attempt to classify vacuum gauges automatically through pre-processing, curve fitting and\n",
    "a comparison of fits via their Mean Square Error.\n",
    "\n",
    "* [Setup](#setup)\n",
    "    * [Environment](#env)\n",
    "    * [Plotting Functions](#plotters)\n",
    "    * [Data Retrieval](#retrieval)\n",
    "* [Preprocessing](#preprocess)\n",
    "    * [Normalization](#normalization)\n",
    "    * [Interpolation](#interpolaton)\n",
    "    * [Masking](#masking)\n",
    "    * [Fourier Transform](#fft)\n",
    "    * [Constrained Inverse Fourier Transform](#ifft)\n",
    "* [Classification](#class)\n",
    "    * [Curve fitting](#fit)\n",
    "    * [Plotting](#plot)\n",
    "    * [All-In-One Classifier](#allin1)\n",
    "    * [Multi-Gauge Plotting](#multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='setup'> Setup </a>\n",
    "### <a id='env'> Environment </a>\n",
    "Run every time to get appropriate libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run BackEnd_Plotters.ipynb\n",
    "%run BackEnd_DataProcessing.ipynb\n",
    "%run BackEnd_K-Neighbour.ipynb\n",
    "%run BackEnd_Analytical_Classifiers.ipynb\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import pytimber\n",
    "import tkinter as tk\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import exponential\n",
    "from scipy.signal import medfilt\n",
    "import ipywidgets as widgets\n",
    "import asyncio\n",
    "\n",
    "import math\n",
    "from scipy import signal\n",
    "db = pytimber.LoggingDB()\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Fitting Approach\n",
    "Since the shape for our simple case is non-linear, we can try to find the optimal parameters to fit the shape of our pressure readings. For this we select a decay function and a 2nd order polynomial. In order to reduce sensitivity to noise, we can pre-proecess the data to eliminate holes and ensure an even-spacing between sucessive data points. \n",
    "\n",
    "Here we achieve this by eliminating high-frequency noise in the Fourier Domain and then returning to the time domain to obtain a smooth function. We can then fit our decay funtion (corresponding to normal behaviour) and our polynomial (corresponding to coupling). If the polynomial has a lower mean square error then we classify the function as exhibiting some form of coupling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='preprocess'> Pre-Processing </a>\n",
    "### <a id='retrieval'> Data retrieval </a>\n",
    "Sample Vacuum Gauge data through PyTimber, returns time (x-axis), pressure_readings (y_axis) and overlays the beam intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_id = \"VGPB.623.4R8.B.PR\"\n",
    "fillNo = 2216\n",
    "\n",
    "pressure_readings,\\\n",
    "time_readings,\\\n",
    "beam_time,\\\n",
    "beam_energy = retrieve_gauge_data(gauge_id, fillNo,show_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='normalization'> Normalization </a>\n",
    "As we are concerned with the shape and form of the pressure rather than the value of the pressure, normalization between 0 and 1 allows for more reasonable comparisons between different gauges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pressure_readings,\\\n",
    "beam_energy = normalize_y(time_readings,\n",
    "                          beam_time,\n",
    "                          pressure_readings,\n",
    "                          beam_energy,\n",
    "                          show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='interpolaton'> Interpolation </a>\n",
    "\n",
    "The pressure values are interpolated to have an equally spaced x-axis and then it is possible to plot them between 0 and 1 without losing information. The choice to normalize between 0 and 1 is less meaningful than the normalization along the y-axis, since this may correspond to different overall durations (and hence more information will be contained for longer durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_readings,pressure_readings,beam_time,beam_energy = interpolate_readings(pressure_readings, time_readings, beam_time, beam_energy,\n",
    "                                   show_plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='masking'> Masking </a>\n",
    "We are only concerned with the pressure response after the intensity has ramped up, since this corresponds to there being bunches in the LHC. Hence we used the maximum of the intensity, which occurs after the LHC is filled to the desired level, as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask,\\\n",
    "threshold = double_threshold_energy_masking(time_readings, \n",
    "                             beam_time,\n",
    "                             beam_energy,\n",
    "                             show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='fft'> Forward Fourier Transform </a>\n",
    "With a normalized, interpolated dataset we can now safely transform to the Fourier Domain to obtain the constituent sine-wave components of our signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_transform, spectrum, deltaT = forward_fourier_transform(time_readings,\n",
    "                                                                 pressure_readings,\n",
    "                                                                 show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ifft'> Constrainted Inverse Fourier Transform </a>\n",
    "Using a manually-tuned threshold, we can remove high-frequency components of the original signal. When selected correctly, these should correspond to noise - though some information will be lost. Once these components are removed, we can apply an inverse transform to return to the time-domain with a new (smooth) function. \n",
    "\n",
    "Note that this approach may suffer from Gibb's Phenomenon at sharp changes in pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_constrained, signal_constrained = filtered_inverse_fourier_transform(pressure_transform,\n",
    "                                                                          deltaT,\n",
    "                                                                          spectrum[0],\n",
    "                                                                          40,\n",
    "                                                                          show_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='class'> Classificaton  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='fit'>Curve fitting: </a>\n",
    "Using the noise-constrained curve from our Inverse Fourier transform, we can now try to fit a decay function as well as a 3rd order polynomail to the signal in order to attempt to classify it. \n",
    "\n",
    "The mean square error is te average distance between the fit and the original signal, a smaller value means it is a better fit overall. Therefore if the signal is decaying (pressure is dropping) as we might expect under normal (non-coupling) circumstances, the decay function should fit better. This can therefore be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\033[1mFit for Constrained Signal\\033[0m: \")\n",
    "poly_fit, decay_fit, coupled = fit_curves(time_constrained,\n",
    "                                signal_constrained,\n",
    "                                mask,\n",
    "                                verbose=True)\n",
    "\n",
    "print(\"\\n\\033[1mFit for Unconstrained Signal\\033[0m: \")\n",
    "poly_fit, decay_fit, coupled = fit_curves(time_readings,\n",
    "                                pressure_readings,\n",
    "                                mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='plot'> Plotting <a id='plot'>\n",
    "Dislay entire fitting scheme on one plot, the curve fits act on the constrained signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_analytical_classifier(time_readings,\n",
    "                           pressure_readings,\n",
    "                           time_constrained,\n",
    "                           signal_constrained,\n",
    "                           poly_fit,\n",
    "                           decay_fit,\n",
    "                           mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='allin1'> All-In-One Curve Fitting Classifier Function </a>\n",
    "Cal entire classification pipeline from one function, allows for classification of many gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_classifier(gauge_id,\n",
    "                      fillNumber=2216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='multi'>Multi Gauge </a>\n",
    "Plot for many gauges extracted from a CSV file, required a probe id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'CompleteProbeCatalogue.csv')\n",
    "limit_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0, max=df.shape[0],\n",
    "    step=1,\n",
    "    description='Limit:',\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "display(limit_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "limit = limit_slider.value\n",
    "for index, row in df.iterrows():\n",
    "    gauge_id = row['Probe ID']\n",
    "    try:\n",
    "        analytical_classifier(gauge_id,row['Fill'])\n",
    "        print(\"Probe %s Fill: %d, label:%s\"%(gauge_id,int(row['Fill']),row['Steepness']))\n",
    "        if index >= limit:\n",
    "            break\n",
    "    except:\n",
    "        print(\"Problem with %s\"%gauge_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for unlabelled gauges using PyTimber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='naive'>Naive Approach: Derivatives </a>\n",
    "Here we take a naive approach to the problem using simple calculus. For the case where there is a second peak after the beam energy ramps up - which suggests some form of coupling - we can check whether there is a 2nd maximum. If there is a 2nd maximum (that is larger than the 1st) then we classify it as coupled.\n",
    "\n",
    "After applying a smoothing function (using a mean convolution), we take the 1st derivative to find all turning points (gradient = 0) and then use the 2nd derivative to confirm whether a particular turning point is a maximum or not. If the 2nd maximum is greater than the 1st, we found coupling. \n",
    "\n",
    "This has several limitations: <br />\n",
    "<pre> 1) A loss of information in smoothing </pre>\n",
    "<pre> 2) Failure to find minima/maxima due to 'holes' in data  </pre>\n",
    "<pre> 3) A sensitivity to our choice of smoothing and neighbour parameters  </pre>\n",
    "The resulting algorithm has a ~60% accuracy for gauges we previously labeled as following this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VG_analyzer_naive(time_data, pressure_data,smoothing=50,neighbours=10,plot=True):\n",
    "    if type(pressure_data) is not np.ndarray: #is not the right type\n",
    "        raise AnalyzerError(\"Analyzer FAILED - Expected numpy.ndarray but got %s\"%type(pressure_data))\n",
    "        \n",
    "    if np.min(pressure_data) != 0 or np.max(pressure_data) != 1: #is not normalized already\n",
    "        pressure_data = ( pressure_data - np.min(pressure_data) ) /  ( np.max(pressure_data) - np.min(pressure_data))\n",
    "    pressure_raw = np.copy(pressure_data)\n",
    "    pressure_data = smooth(pressure_data,smoothing) \n",
    "\n",
    "    pressure_1st_deriv = np.diff(pressure_data, n=1) #1st order\n",
    "    pressure_1st_deriv = smooth(pressure_1st_deriv,smoothing) #w. smoothing\n",
    "    \n",
    "    pressure_2nd_deriv = np.diff(pressure_data, n=2) #2nd order\n",
    "    pressure_2nd_deriv = smooth(pressure_2nd_deriv,smoothing) #w. smoothing\n",
    "    \n",
    "    turning_points = {\"Maximum\":[],\"Minimum\":[],\"Saddle_Point\":[]}\n",
    "    try:\n",
    "        discovery = -1\n",
    "        # Check neighbouring points to enforce a minimum width\n",
    "        for i in range(neighbours,len(pressure_1st_deriv)-neighbours):\n",
    "            if i <= (discovery + neighbours): # Only find a turning point once!\n",
    "                continue      \n",
    "            if ( (pressure_1st_deriv[i-neighbours:i] > 0).all() and (pressure_1st_deriv[i+1:i+neighbours] < 0).all() ) or \\\n",
    "               ( (pressure_1st_deriv[i-neighbours:i] < 0).all() and (pressure_1st_deriv[i+1:i+neighbours] > 0).all() ):   \n",
    "                if pressure_2nd_deriv[i] == 0:\n",
    "                    turning_points[\"Saddle_Point\"].append(i)\n",
    "                elif pressure_2nd_deriv[i] > 0:\n",
    "                    turning_points[\"Minimum\"].append(i)\n",
    "                else:\n",
    "                    turning_points[\"Maximum\"].append(i)  \n",
    "                discovery = i\n",
    "    except:\n",
    "        raise AnalyzerException(\"Index Out of Bounds, dataset size %s too small for the neighbour value %s\"%(len(pressure_data,neighbours)))\n",
    "    display(turning_points)\n",
    "    \n",
    "    if plot:\n",
    "        f, ax1 = plt.subplots(1)\n",
    "        ax1.plot(time_data,pressure_raw,color='black',label=\"Raw\")\n",
    "        ax1.plot(time_data,pressure_data,alpha=0.7,label=\"Smoothed\")\n",
    "        pytimber.set_xaxis_date()\n",
    "        ax1_1 = ax1.twinx()\n",
    "        ax1_1.scatter(time_data[:-1],pressure_1st_deriv,color='orange',label=\"1st Deriv\")\n",
    "        ax1_1.scatter(time_data[:-2],pressure_2nd_deriv,color='r',label=\"2nd Deriv\")\n",
    "        for key in turning_points:\n",
    "            for loc in turning_points[key]:\n",
    "                ax1_1.axvline(x=time_data[loc],color='black')\n",
    "        f.legend()\n",
    "        f.show()\n",
    "    for index in range(0,len(turning_points[\"Maximum\"])-1):\n",
    "        if len(turning_points[\"Maximum\"]) == 1:\n",
    "            break\n",
    "        elif turning_points[\"Maximum\"][index] < turning_points[\"Maximum\"][index+1]:\n",
    "            print(\"COUPLING DETECTED: Successive maximum [%d/%d,%d/%d] is larger than previous\"%(turning_points[\"Maximum\"][index],len(pressure_data),turning_points[\"Maximum\"][index+1],len(pressure_data)))\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "class AnalyzerError(Exception):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
