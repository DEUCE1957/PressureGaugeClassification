{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackEnd for Classifiers\n",
    "Provides generalized methods for training, testing and saving classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-2-2fe9d552a2d4>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2fe9d552a2d4>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    warnings.filterwarnings(\"ignore\", category=DeprecationWarning,FutureWarning)\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Packages\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Utility Packages\n",
    "import re\n",
    "import inspect\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "import types\n",
    "\n",
    "# Data Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytimber\n",
    "db = pytimber.LoggingDB()\n",
    "\n",
    "# Decorative Packages\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lockout():\n",
    "    resp = input(\"Are you sure you want to reset? 1: Yes, 0: No\")\n",
    "    while True:\n",
    "        if re.match(\"\\d+\",resp):\n",
    "            resp = int(resp)\n",
    "            if resp == 1:\n",
    "                return 2\n",
    "            if resp == 0:\n",
    "                return 0\n",
    "        print(\"Please type a number, 0 or 1\")\n",
    "        resp = input(\"Are you sure you want to reset? 1: Yes, 0: No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data from Catalogue\n",
    "Use probe ID and Fill Numbers from a CSV-stored catalogue to retrieve Probe Data. <br>\n",
    "Probe data is cached to the local data sub-directory and is returned as a Pandas DataFrame.\n",
    "\n",
    "**bin_spec** Is a variable that specifies how many bins to use for generating features:\n",
    "* A tuple of length 2 e.g. (3,6) specifies the lower bound 3 and upper bound 6 with step 1\n",
    "* A tuple of length 3 e.g. (2,12,3) specifies the lower bound 2and upper bound 12 with step 3\n",
    "* An integer specifies to generate only 1 cut\n",
    "\n",
    "Note for future development: If a new feature set is required, pass a different feature function <br>\n",
    "E.g. Fourier Coeffecients, Wavelet Transform terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catalogue(catalogue_name):\n",
    "    path = os.path.join(os.getcwd(),\"data\",\"catalogue\",\"{}_Catalogue.csv\".format(catalogue_name))\n",
    "    if os.path.isfile(path):\n",
    "        catalogue = pd.read_csv(path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"{} does not appear to exist\".format(path))\n",
    "    if not isinstance(catalogue, pd.DataFrame): raise TypeError(\"Catalogue provided is not a Pandas Dataframe\")\n",
    "    if not all(column in catalogue.columns.values for column in [\"Probe ID\",\"Response\",\"Fill\"]):\n",
    "        raise ValueError(\"Expected to find 'Probe ID','Response' and'Fill' in Catalogue header\")\n",
    "    return catalogue\n",
    "\n",
    "def process_feature_spec(feature_spec):\n",
    "    if type(feature_spec) is numpy.ndarray:\n",
    "        return feature_spec\n",
    "    if type(feature_spec) is tuple:\n",
    "        if len(feature_spec) == 2:\n",
    "            return np.arange(feature_spec[0],feature_spec[1],1)\n",
    "        elif len(feature_spec) == 3:\n",
    "            return np.arange(feature_spec[0],feature_spec[1],feature_spec[2])\n",
    "        else:\n",
    "            raise ValueError(\"Too many values to unpack from bins specification: {}\".format(feature_spec))\n",
    "    if type(feature_spec) == int or type(feature_spec) == np.int64:\n",
    "        return np.atleast_1d(feature_spec) #1D array so that it is Iterable\n",
    "    else:\n",
    "        raise ValueError(\"Bin Specification {} is of type {} but must be a tuple or int\".format(feature_spec,type(feature_spec)))\n",
    "    \n",
    "def generate_data_from_catalogue(catalogue_name, feature_spec=4, reset=0, feature_generator = bin_data, verbose=0):\n",
    "    catalogue = get_catalogue(catalogue_name)\n",
    "    if verbose==1: print(\"{0}Catalogue {2}{1}: {3}\".format(color.BOLD,color.END,catalogue_name,catalogue.shape))\n",
    "    \n",
    "    if reset == 1: reset = lockout() # To prevent accidental overwritting\n",
    "        \n",
    "    # Binning Setup\n",
    "    feature_spec = process_feature_spec(feature_spec)\n",
    "    \n",
    "    # Ignore UNDETERMINED gauges\n",
    "    catalogue = catalogue[catalogue.Response != \"UNDETERMINED\"]\n",
    "    if verbose == 1: print(\"Ignoring UNDETERMINED gauges in Catalogue, New Shape: {}\".format(catalogue.shape))\n",
    "    catalogue.reset_index(inplace=True, drop=True) # Start Index from 0\n",
    "    \n",
    "    # Display Progress\n",
    "    progressBar = IntProgress(min=0, max=catalogue.shape[0],description='Progress:',bar_style='info') \n",
    "    display(progressBar)\n",
    "    \n",
    "    # Output\n",
    "    # Rows = No. Of Valid Probes, Columns = No.of features\n",
    "    Xs = [np.zeros((catalogue.shape[0],no_of_features)) for no_of_features in feature_spec]\n",
    "    y = catalogue.Response.replace(to_replace=['NORMAL','COUPLED'],value=[0,1])\n",
    "    lookup = pd.concat((catalogue['Probe ID'],catalogue['Fill']),axis=1)\n",
    "\n",
    "    for index, row in catalogue.iterrows():\n",
    "        gauge_id, fill_no, response = row['Probe ID'], row['Fill'], row['Response']\n",
    "        gauge_id = gauge_id.replace(\".\", \"-\")\n",
    "        \n",
    "        # Setup Paths\n",
    "        file_name = \"Probe_{0}_Fill{1}.p\".format(gauge_id,str(fill_no))\n",
    "        folder = os.path.join(os.getcwd(),\"data\",\"probes\",str(fill_no),gauge_id) \n",
    "        file_path = os.path.join(folder,file_name)\n",
    "        \n",
    "        feature_folder = os.path.join(folder,catalogue_name)\n",
    "        \n",
    "        # Create Folder if it does not exist already\n",
    "        if not os.path.exists(feature_folder): os.makedirs(feature_folder)\n",
    "        \n",
    "        # If file already exists, load it\n",
    "        if os.path.isfile(file_path) and reset == 0:\n",
    "            if verbose == 2: print(\"Loading existing data for {} in Fill {}\".format(gauge_id,fill_no))\n",
    "            # Safely close and open files using with...\n",
    "            with open(file_path,\"rb\") as pgd_file:\n",
    "                pgd = pickle.load(pgd_file)\n",
    "                # Only load the Processed Gauge Data (pgd) once, then use it for each feature\n",
    "                for no_of_features in feature_spec:\n",
    "                    feature_file_name = \"{}_Intervals_Gauge_{}.p\".format(no_of_features,gauge_id)\n",
    "                    feature_path = os.path.join(feature_folder,feature_file_name)\n",
    "                    if os.path.isfile(feature_path):\n",
    "                        with open(feature_path,\"rb\") as feature_file:\n",
    "                            features = pickle.load(feature_file)\n",
    "                            Xs[no_of_features-feature_spec[0]][index] = features\n",
    "                    else:\n",
    "                        with open(feature_path,\"wb\") as feature_file:\n",
    "                            features = feature_generator(pgd.pressure_readings[pgd.mask],no_of_features)\n",
    "                            Xs[no_of_features-feature_spec[0]][index] = features\n",
    "                            pickle.dump(features, feature_file)\n",
    "        else:\n",
    "            if verbose == 2: print(\"Saving new data for {} in Fill {}\".format(gauge_id,fill_no))\n",
    "            pgd = processed_gauge_data(row['Probe ID'], fill_no)\n",
    "            pgd.generate_data() # Retrieve pressure readings, mask etc.\n",
    "            with open(file_path,\"wb\") as pgd_file:\n",
    "                pickle.dump(pgd, pgd_file)\n",
    "                for no_of_features in feature_spec:\n",
    "                    try:\n",
    "                        features = feature_generator(pgd.pressure_readings[pgd.mask],no_of_features)\n",
    "                    except:\n",
    "                        if verbose == 2: print(\"Could not generate data for {0} in fill {1}.\".format(gauge_id,fill_no))\n",
    "                        continue\n",
    "                    feature_file_name = \"{}_Intervals_Gauge_{}.p\".format(no_of_features,gauge_id)\n",
    "                    feature_path = os.path.join(feature_folder,feature_file_name)\n",
    "                    with open(feature_path, \"wb\") as feature_file:\n",
    "                        pickle.dump(features, feature_file)\n",
    "                        Xs[no_of_features-feature_spec[0]][index] = features\n",
    "        \n",
    "        progressBar.value += 1\n",
    "    if progressBar.value != y.shape[0]:\n",
    "        print(\"{} probes failed to load from Catalogue. Use verbose=2 to find them.\".format(y.shape[0]-progressBar.value))\n",
    "        return None, None, None\n",
    "    else:\n",
    "        datasets = {} # Map no.of features to dataset dataFrame\n",
    "        for X in Xs:\n",
    "            X = pd.DataFrame(X,columns=[\"Bin\"+str(i) for i in range(X.shape[1])])\n",
    "            dataset = pd.concat((X,y,lookup),axis=1)\n",
    "            datasets[X.shape[1]] = dataset\n",
    "            \n",
    "            folder=os.path.join(os.getcwd(),\"data\",\"datasets\",catalogue_name)\n",
    "            dataset_name = \"{}_features_{}.csv\".format(X.shape[1],catalogue_name)\n",
    "            dataset_path = os.path.join(folder,dataset_name)\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "                \n",
    "            if os.path.isfile(dataset_path) and reset == 0:\n",
    "                if verbose == 1: print(\"Dataset already exists\")\n",
    "            else:\n",
    "                dataset.to_csv(dataset_path, index=False, header = True)\n",
    "                \n",
    "            if verbose == 2: display(dataset.head())\n",
    "        return datasets\n",
    "            \n",
    "# Close files! CHECK\n",
    "# Intervals can be a single number of a tuple with a lower and upper limit CHECK\n",
    "    # if type(intervals) is tuple\n",
    "# Use name of catalogue as name of dataset handle CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_datasets(catalogue_name, feature_spec=4, reset=0, feature_generator = bin_data, verbose=0):\n",
    "    catalogue = get_catalogue(catalogue_name)\n",
    "    if verbose==1: print(\"{0}Catalogue {2}{1}: {3}\".format(color.BOLD,color.END,catalogue_name,catalogue.shape))\n",
    "        \n",
    "    if reset == 1: reset = lockout() # To prevent accidental overwritting\n",
    "        \n",
    "    # Binning Setup\n",
    "    feature_spec = process_feature_spec(feature_spec)\n",
    "    \n",
    "    datasets = {}\n",
    "    if reset == 0:\n",
    "        for no_of_features in feature_spec:\n",
    "            # Setup Paths\n",
    "            folder=os.path.join(os.getcwd(),\"data\",\"datasets\",catalogue_name)\n",
    "            dataset_name = \"{}_features_{}.csv\".format(no_of_features,catalogue_name)\n",
    "            dataset_path = os.path.join(folder,dataset_name)\n",
    "            if os.path.isfile(dataset_path):\n",
    "                dataset = pd.read_csv(dataset_path)\n",
    "                datasets[no_of_features] = dataset\n",
    "                if verbose == 2: display(dataset.head())\n",
    "    if len(datasets) == len(feature_spec):\n",
    "        return datasets\n",
    "    elif datasets == {}:\n",
    "        return generate_data_from_catalogue(catalogue_name, feature_spec, reset, feature_generator, verbose)\n",
    "    else:\n",
    "        for no_of_features in (no_of_features for no_of_features in feature_spec if no_of_features not in datasets.keys()):\n",
    "            datasets[no_of_features] = generate_data_from_catalogue(catalogue_name, no_of_features, reset, verbose)[no_of_features]\n",
    "        return datasets\n",
    "\n",
    "# Check entire feature_spec Range\n",
    "# If entire range is free: load full feature_spec as one\n",
    "# If only a few are free: load individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Grid Search\n",
    "Step-by-step Grid Search used for Diagnostics and conceptual awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X,y, seed = 0, verbose = 0):\n",
    "    X_train, X_test, X_train_labels, X_test_labels = train_test_split( X, y,\n",
    "                                                                      test_size=0.2, random_state=seed)\n",
    "    if verbose >= 1: print (\"\"\">>>> {0}Splitting Dataset{1} <<<<\\n{0}Train set:{1} Data {2} Labels {3}\n",
    "                            {0}Test set:{1}  Data {4} Labels {5}\"\"\".format(color.BOLD,color.END,\n",
    "                                                                        X_train.shape,X_train_labels.shape,\n",
    "                                                                        X_test.shape,X_test_labels.shape))\n",
    "    return X_train, X_test, X_train_labels, X_test_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_sets(X_train, X_train_labels, verbose = 0,\n",
    "                        cv_splitter = None, scorer = None, classifier = None, seed = 0):\n",
    "    if scorer is None: scorer = metrics.make_scorer(pick_tool(\"Scorer\",\"_score\",moduleName=\"metrics\"))\n",
    "    if isinstance(scorer, types.FunctionType): scorer = metrics.make_scorer(scorer)\n",
    "    if cv_splitter is None: \n",
    "        cv_splitter = pick_tool(\"CV-Splitter\",\"(Fold|Split|Leave)\",moduleName=\"model_selection\")(test_size=0.2,train_size=0.8,random_state=seed)\n",
    "    else:\n",
    "        cv_splitter= cv_splitter(test_size=0.2,train_size=0.8,random_state=seed)\n",
    "    if classifier is None: \n",
    "        classifier = pick_tool(\"Machine Learning Algorithm\",\"Classifier\",moduleName=None)\n",
    "    average_cv_score = np.mean(cross_val_score(classifier(), X_train, X_train_labels, cv=cv_splitter, scoring = scorer))\n",
    "    print(\"\"\">>>> Cross Validation <<<<\n",
    "    {0}Average CV score{1}: {2}\n",
    "    {0}Train Set{1}: {3}\n",
    "    {0}Validation set{1}: {4}\"\"\".format(color.BOLD,color.END,average_cv_score,\n",
    "                                       cv_splitter.train_size * len(X),\n",
    "                                       cv_splitter.test_size * len(X)))\n",
    "    if verbose >= 1:\n",
    "        for training_indices, validation_indices in cv_splitter.split(X,y):\n",
    "            display(np.sort(training_indices)[0:5])\n",
    "            display(np.sort(validation_indices)[0:5])\n",
    "    return cv_splitter, average_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_parameter_manualy(X_train, X_train_labels, scorer = None, cv_splitter=None, classifier = None, seed = 0, verbose=1, show_plot=True):\n",
    "    if scorer is None: scorer = pick_tool(\"Scorer\",\"_score\",moduleName=\"metrics\")\n",
    "    if cv_splitter is None:\n",
    "        cv_splitter = pick_tool(\"CV-Splitter\",\"(Fold|Split|Leave)\",moduleName=\"model_selection\")\n",
    "    cv_splitter = cv_splitter(test_size=0.2,train_size=0.8,random_state=seed)\n",
    "    if classifier is None: classifier = pick_tool(\"Machine Learning Algorithm\",\"Classifier\",moduleName=None)\n",
    "        \n",
    "    # Provide Help using the classifier's DocString\n",
    "    if verbose >= 1: print(\">>>>{0}DOCUMENTATION{1}<<<<\\n{2}\\n>>>>{0}END DOCUMENTATION{1}<<<<\".format(color.BOLD,color.END,re.split(\"-{8,}\",classifier.__doc__)[1]))\n",
    "    \n",
    "    # Find and Select 1 parameter to vary from the Classifier\n",
    "    parameters = inspect.getfullargspec(classifier).args\n",
    "    parameters.remove(\"self\")\n",
    "    for i in range(0,len(parameters)): print(\"{0}: {1}\".format(i,parameters[i]))\n",
    "\n",
    "    resp = input(\"Select Parameter by typing its associated number >>>\")\n",
    "    while True:\n",
    "        if re.match(\"\\d+\",resp):\n",
    "            if int(resp) in range(0,len(parameters)):\n",
    "                param_name = parameters[int(resp)]\n",
    "                param_range = set_param_limits(param_name,classifier)\n",
    "                break\n",
    "        resp = input(\"Select Parameter by typing its associated number >>>\")\n",
    "    \n",
    "    \n",
    "    progessBar = IntProgress(min=0, max=cv_splitter.get_n_splits(X_train)*len(param_range),description='Progress:',bar_style='info') # instantiate the bar\n",
    "    display(progessBar)\n",
    "    \n",
    "    best_score = -1\n",
    "    scores = np.zeros((len(param_range),cv_splitter.get_n_splits(X)))\n",
    "    best_param_setting = -1\n",
    "    \n",
    "    param_count = 0\n",
    "    for val in param_range: \n",
    "        split_count = 0\n",
    "        for training_indices, validating_indices in cv_splitter.split(X_train,X_train_labels):\n",
    "            X_build,X_build_labels = X_train.iloc[training_indices], X_train_labels.iloc[training_indices]\n",
    "            X_valid, X_valid_labels = X_train.iloc[validating_indices], X_train_labels.iloc[validating_indices]\n",
    "            model = classifier(**{param_name:val}).fit(X_build,X_build_labels)\n",
    "            xhat = model.predict(X_valid)\n",
    "            if verbose >= 2:\n",
    "                print(\"\"\"{0}{2} Using {3} {4}{1}\n",
    "                Train Set {5}: {6}\n",
    "                Validation Set {5}: {7}\n",
    "                \"\"\".format(color.BOLD,color.END,\n",
    "                           classifier.__name__,param_name, val,\n",
    "                           scorer.__name__,\n",
    "                           scorer(y_true=X_build_labels,y_pred=model.predict(X_build)),\n",
    "                           scorer(X_valid_labels,xhat)))\n",
    "            score = scorer(y_true=X_valid_labels,y_pred=xhat)\n",
    "            scores[param_count,split_count] = score\n",
    "            split_count += 1\n",
    "            progessBar.value += 1\n",
    "       \n",
    "        if np.mean(scores[param_count])>best_score:\n",
    "            best_score = np.mean(scores[param_count])\n",
    "            best_param_setting = val\n",
    "            \n",
    "        param_count += 1\n",
    "            \n",
    "    mean_score = np.mean(scores,axis=1)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(\"\"\"\n",
    "        {0}Best {2}{1}: {3}\n",
    "        {0}Best {4}{1}: {5} \n",
    "        {0}Mean {2}{1}:\"\"\".format(color.BOLD,color.END,\n",
    "                   scorer.__name__,\n",
    "                   np.max(mean_score), param_name,\n",
    "                   best_param_setting))\n",
    "        display(pd.DataFrame(mean_score,index=param_range,columns=[param_name]).transpose())\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.figure()\n",
    "        plt.plot(param_range,mean_score,'g')\n",
    "        plt.ylabel(scorer.__name__)\n",
    "        plt.xlabel(param_name)\n",
    "        plt.xticks(param_range)\n",
    "        plt.xlim((param_range[0],param_range[-1]))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return best_score, best_param_setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Grid Search\n",
    "Find the best parameters for maximizing any desired score using Grid Search cross validation. <br>\n",
    "Can pre-specify Classifier, Scorer, CV_Splitter, Seed and the Parameter Grid but others full <br> user-controlled customization of each of these settings. <br>\n",
    "Note that suppling Datasets is NOT optional, these are needed to perform the GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_tool(toolType,searchTerm,moduleName=None):\n",
    "    if moduleName is None:\n",
    "        moduleName = pick_tool(\"Algorithm Type\",\"(neural_network|neighbors|svm|gaussian_process|tree|ensemble|naive_bayes|discriminant_analysis)\",\"sklearn\").__name__\n",
    "        exec(\"from sklearn import {0}\".format(moduleName.split(\".\")[1]))\n",
    "        \n",
    "    print(\">>>> Pick a {} <<<<\".format(toolType))\n",
    "    tool_list = [tool for tool in dir(eval(moduleName)) if re.search(searchTerm,tool)] \n",
    "    for index in range(0,len(tool_list),6):\n",
    "        print(f\"\".join(f'{pair}\\n' for pair in [(i,tool_list[i]) for i in range(index,min(len(tool_list), index+5))]))\n",
    "    resp = input(\"Type the Index of the {}>>>\".format(toolType))\n",
    "    while True:\n",
    "        try:\n",
    "            match = int(re.match(\"\\d+\",resp)[0])\n",
    "        except TypeError:\n",
    "            match = -1 #Invalid\n",
    "        if 0 <= match < len(tool_list):\n",
    "            print(\"Selected {} as {}\".format(tool_list[match],toolType))\n",
    "            tool = tool_list[match]\n",
    "            break\n",
    "        resp = input(\"Type the Index of the {}>>>\".format(toolType))\n",
    "    print(\"{}.{}\".format(moduleName,tool))\n",
    "    return eval(\"{}.{}\".format(moduleName,tool))\n",
    "\n",
    "def set_param_limits(param_name, classifier):\n",
    "    while True:\n",
    "        limit = input(\"'{}' - Set Limits (press enter to use default) >>>\".format(param_name))\n",
    "        if limit == \"\":\n",
    "            return None\n",
    "        try:\n",
    "            limit = eval(limit)\n",
    "            if isinstance(limit, collections.Sequence) and type(limit) is not str:\n",
    "                for elt in limit: classifier(**{param_name:elt})\n",
    "            else:\n",
    "                classifier(**{param_name:limit})\n",
    "        except TypeError:\n",
    "            print(\"'{}' is not of a valid type for '{}'\".format(limit,param_name))\n",
    "            continue\n",
    "        except (NameError, SyntaxError, ValueError) as e:\n",
    "            print(\"{} contains invalid syntax, check carefully\".format(limit))\n",
    "            continue\n",
    "        if not isinstance(limit, collections.Sequence) and not type(limit) is np.ndarray or type(limit) is str:\n",
    "            limit = [limit]\n",
    "        break\n",
    "    return limit\n",
    "\n",
    "def find_best_parameters(datasets, param_grid = None, classifier = None, cv_splitter = None, scorer=None, seed = 0, verbose=0):\n",
    "    if scorer is None: scorer = metrics.make_scorer(pick_tool(\"Scorer\",\"_score\",moduleName=\"metrics\"))\n",
    "    if isinstance(scorer, types.FunctionType): scorer = metrics.make_scorer(scorer)\n",
    "    if cv_splitter is None: \n",
    "        cv_splitter = pick_tool(\"CV-Splitter\",\"(Fold|Split|Leave)\",moduleName=\"model_selection\")\n",
    "    cv_splitter = cv_splitter(test_size=0.2,train_size=0.8,random_state=seed)\n",
    "    if classifier is None: classifier = pick_tool(\"Machine Learning Algorithm\",\"Classifier\",moduleName=None)\n",
    "    best_params, best_no_of_features, best_score = None, -1, -1\n",
    "    \n",
    "    # Provide Help using the classifier's DocString\n",
    "    print(\">>>>{0}DOCUMENTATION{1}<<<<\\n{2}\\n>>>>{0}END DOCUMENTATION{1}<<<<\".format(color.BOLD,color.END,re.split(\"-{8,}\",classifier.__doc__)[1]))\n",
    "    \n",
    "    if param_grid is None:\n",
    "        parameters = inspect.getfullargspec(classifier).args\n",
    "        parameters.remove(\"self\")\n",
    "        param_grid = {param_name: set_param_limits(param_name,classifier) for param_name in parameters}\n",
    "    filtered_param_grid = {k: v for k, v in param_grid.items() if v is not None}\n",
    "        \n",
    "    for no_of_features, dataset in datasets.items():\n",
    "        X,y,lookup = dataset.iloc[:,0:dataset.shape[1]-3],dataset.Response,dataset.iloc[:,-2:]\n",
    "        X_train, X_test, X_train_labels, X_test_labels = split_dataset(X,y, seed = seed)\n",
    "        generated_models = GridSearchCV(classifier(), filtered_param_grid, scoring=scorer, cv=cv_splitter, verbose=verbose)\n",
    "        generated_models.fit(X_train,X_train_labels)\n",
    "        if verbose >= 2: print(\"Best Parameters for {} features: \\n {}\".format(no_of_features,generated_models.best_params_))\n",
    "        if (generated_models.best_score_ > best_score):\n",
    "            best_score = generated_models.best_score_\n",
    "            best_params = generated_models.best_params_\n",
    "            best_no_of_features = no_of_features\n",
    "    return best_no_of_features, best_params, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Use unseen Testing Set to evaluate the performance of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(datasets, testing_dataset = None, classifier = None, no_of_features = None, best_params = None, scorer = None, seed = 0, verbose = 0):\n",
    "    if scorer is None: scorer = pick_tool(\"Scorer\",\"_score\",moduleName=\"metrics\")\n",
    "        \n",
    "    if classifier is None: classifier = pick_tool(\"Machine Learning Algorithm\",\"Classifier\",moduleName=None)\n",
    "        \n",
    "    if no_of_features is None or best_params is None: # Did not yet optimize the model\n",
    "        no_of_features, best_params, best_score = find_best_parameters(datasets,classifier=classifier,\n",
    "                                                                            cv_spliter=None,scorer=scorer,\n",
    "                                                                            seed = seed, verbose = verbose)\n",
    "    \n",
    "    dataset = datasets[no_of_features - min(datasets.keys())]\n",
    "    X,y,lookup = dataset.iloc[:,0:dataset.shape[1]-3],dataset.Response,dataset.iloc[:,-2:]\n",
    "\n",
    "    if testing_dataset is None:\n",
    "        X_train, X_test, X_train_labels, X_test_labels = split_dataset(X,y, seed = seed)\n",
    "        test_lookup = lookup.iloc[X_test_labels.index]\n",
    "    else:\n",
    "        X_train, X_train_labels = X,y\n",
    "        \n",
    "        X_test,X_test_labels,test_lookup = testing_dataset.iloc[:,0:testing_dataset.shape[1]-3],testing_dataset.Response,testing_dataset.iloc[:,-2:]\n",
    "\n",
    "    if verbose >= 1: print(\"X:{} y:{}\".format(X.shape,y.shape))\n",
    "    classifier = classifier(**best_params)\n",
    "    if verbose >= 2: print(\"Optimized with {} features and parameters:\\n{}\".format(no_of_features,best_params))\n",
    "    model = classifier.fit(X_train,X_train_labels)\n",
    "    yhat = model.predict(X_test)\n",
    "    xhat = model.predict(X_train)\n",
    "    print(\"{0}Train Set {2} {1}: {3}\".format(color.BOLD,color.END,scorer.__name__,scorer(y_true=X_train_labels,y_pred=xhat)))\n",
    "    print(\"{0}Test Set {2} {1}: {3}\".format(color.BOLD,color.END,scorer.__name__,scorer(y_true=X_test_labels,y_pred=yhat)))\n",
    "    if verbose >= 1: print(\">>>>{} Confusion Matrix {}<<<<\\nTrue Positive,False Negative\\nFalse Positive,True Negative\".format(color.BOLD,color.END))\n",
    "    cnf_matrix = get_confusion_matrix(X_test_labels,yhat)\n",
    "    print(\"{}Classification Report{}\".format(color.BOLD,color.END))\n",
    "    print(classification_report(X_test_labels, yhat))\n",
    "          \n",
    "    return model\n",
    "\n",
    "def get_confusion_matrix(y_test_labels,yhat,normal=False):\n",
    "    cnf_matrix = confusion_matrix(y_test_labels, yhat)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[\"Normal\",\"Anomalous\"], normalize= normal,  title='Confusion matrix')\n",
    "    \n",
    "    return cnf_matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Handling\n",
    "## Saving Model to File\n",
    "Preserve a trained model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, best_no_features):\n",
    "    print(\"{}Model Desc:{}\\n {}\".format(color.BOLD,color.END,model))\n",
    "    print(\"Please enter a name for your model\\n%sNote%s: the no.of.features will be appended)\"%(color.UNDERLINE,color.END))\n",
    "    folder = os.path.join(os.getcwd(),\"data\",\"models\",type(model).__name__)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    while True:\n",
    "        resp = input(\">>>\")\n",
    "        try:\n",
    "            saved_model = joblib.dump(model, os.path.join(folder,str(resp)+\"_\"+str(best_no_features)+'.joblib'))\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Successfully saved model to {}\".format(str(resp)+\"_\"+str(best_no_features)+'.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from File\n",
    "Load a previously saved model to use for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_directory = os.path.join(os.getcwd(),\"data\",\"models\")\n",
    "    \n",
    "    valid_files = {}\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(model_directory, topdown=True):\n",
    "        for i in range(0,len(files)):\n",
    "            print(\"{}: {}\".format(count,files[i]))\n",
    "            valid_files[count] = (root,files[i])\n",
    "            count += 1\n",
    "            \n",
    "    #print(\"{}: {}\".format(i,valid_files[i]) for i in range(0,len(valid_files)))\n",
    "    \n",
    "    while True:\n",
    "        resp = input(\"Type the number associated with the model you would like to load\")\n",
    "        if re.match(\"\\d+\",resp):\n",
    "            if int(resp) in valid_files.keys():\n",
    "                file_path = os.path.join(*valid_files[int(resp)])\n",
    "                break\n",
    "        print(\"Please select one of the listed file names by number\")\n",
    "        \n",
    "    model = joblib.load(file_path)\n",
    "    print(\">>>{}Model Loaded{}<<<\\n {}\".format(color.BOLD,color.END,model))\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
